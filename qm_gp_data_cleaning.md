## Cleaning the: 'Total Income of DCMS-Funded Cultural Organisations 2018/2019' dataset

The dataset can be found [here](https://www.gov.uk/government/statistics/total-income-of-dcms-funded-cultural-organisations-201819)

---

#### Several notes about the data:

1. Explanation of the field names:
  - **fundraising_income** = The amount of fundraising income (charitable giving) generated by DCMS-funded cultural organisations (£)
  - **amount_gia** = The amount of Grant-in-Aid given to DCMS-funded cultural organisations (£)
  - **value_donated_objects** = The value of objects donated to the DCMS-funded cultural organisations (£)
  - **ratio_fundraising_to_gia** = Ratio of fundraising income to Grant-in-Aid (ratio)
  - **total_income** = Total income of DCMS-funded cultural organisations (£)
2. The numbers in the following fields are all raw numbers represented in £ (some are particularly high i.e. (£)xx,xxx,xxx:
  - fundraising_income
  - amount_gia
  - value_donated_objects
  - total_income
3. The values for 'ratio_fundraising_to_gia' are percentages i.e 0.18 = 18%, there are some values such as 4.06 which = 406%
4. There were several values read in incorrectly when the data was converted to csv so I manually altered them (using the original Charitable Giving Indicators 2017/18 dataset)
5. I dropped the grand_total data from the dataframe after the join since it was of no use 
6. **Important note:** The 2009/2010 'value_donated_objects' for the Science Museum Group is *554* - this is a **major** outlier. I had a look at the original dataset and the dataset where I aggregated the 'National Museum of Science and Industry' and 'Museum of Science and Industry Manchester' together to create the Science Museum Group data and they were both 554 (I suspect that there is a mistake in the original dataset where the additional 000 was not added onto the end of the data since all the records are in the thousands plus I doubt they had donated objects that summed to £554 for the entire year! I've kept the 554 in the dataset but it may be that you want to drop it in your analysis. 
7. I have kept the 'year' field as the *object* datatype since they represent the financial year and aren't a specific date
8. I have kept 'Historic England' in the dataset but it should probably be dropped since it only has total_income data for 2017-2019 since the data before was deemed (by the dataset collectors) as 'incomparable' so they removed all previous total_income data. I'll leave it to those who are analysing the data to decide what to do with it in case you wanted to look at a specifc year between 2017-2019.
9. I have kept in all the organisations including the *'Arts Council England'* organisation - From my research this museum group funds hundreds of museums/galleries in England which explains why it has the highest total_income values etc for the fields. It will depend on the museum monthly visitors dataset and how they are grouped, but it is likely this organisation may need dropping from the dataset too. 

---
## Cleaning
----
### Stage 1

1. Downloaded the dataset and loaded it in Excel (was an xlsx file with 5 tables)
2. For each table, I converted them into a separate csv file. 
3. I removed any symbols such as the '-' for no values, '%' for the percentage table, and the numbers after several of the organisation names as these were referencing additional notes in the excel spreadsheet.
4. I also converted the values to numbers in excel because when I read in the csv files the first time they were categorised as objects and were not the correct numbers.

---
### Stage 2

I then read each of the csv files in Jupyter.

There were several alterations I needed to make:
1. I used the 'melt' function to alter the dataframe to a wide to long format
2. I renamed the field names so that they were all the same and would make the join easier
3. I dropped all NaN rows that had appeared in the 'organisation' field

---
### Stage 3

1. The next step is to join the dataframes together to create 1 dataframe.
2. I joined each dataframe to a new dataframe called "museum_funding".
3. The *Science Museum Group* aggregated the 'National Museum of Science and Industry' & 'Museum of Science and Industry Manchester' data from 2012l, therefore in the original dataset the Science Museum Group had missing values for 2008/2009, 2009/2010 and 2010/2011. To complete the Science Museum Group data, I decided to fill in the missing data with the aggregations of the 'National Museum of Science and Industry' and 'Museum of Science and Industry Manchester' with their 2008-2012 data. This was what was done from 2012 onwards as these organisations were aggregated into the Science Museum Group.
4. I dropped the following 3 records in the 'organisations' field:
- 'Grand Total' was dropped since these records contained the sums of all the organisations and would mess up later analysis
- 'National Museum of Science and Industry' & 'Museum of Science and Industry Manchester' were both dropped as they were aggregated to the 'Science Museum Group' data. They were already aggregated from 2011/2012 onwards in the original dataset but for ease in analysis it made sense to aggregate the 2008/09, 2009/10 and 2010/11 values to the missing 'Science Museum Group' so that we had complete records for the Science Museum Group for all the years in the dataset. 

---
### Stage 4

Having a look through the revised dataset:

**Initial observations to keep in mind for analysis:**
- The museum_funding.info() shows:
  - There are 13 NaN values in the 'total_income' and 'total_fundraising' field. The majority of these are for the 'Historic England' organisation (see comments in the 'Several notes about the data'). The rows containing these NaN values are *identical*.
  - There are 12 NaN values in the 'amount_gia' field. The rows are identical to the rows with the NaN values for 'total_income' and 'total_fundraising' except for Historic England's 2014/2015 row which has an anomaly record of £80,000,000 This may need removing for analysis. 
  - There are a lot of NaN values in the 'value_donated_objects' field - this field may not be relevant for analysis anyway and can be dropped since it is missing so many values. 
  - Historic England and the English Heritage Trust had quite a few NaN values.

 

